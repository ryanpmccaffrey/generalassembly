{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Intro to Dimensionality Reduction\n",
    "Week 8 | Lesson 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Understand the motivations for dimensionality reduction\n",
    "- Follow the logical workflow behind dimensionality reduction\n",
    "- Describe the basic intuition of Principal Component Analysis\n",
    "- Calculate eigenvectors and eigenvalues for use in Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Have a working understand of scikit learn and numpy\n",
    "- Be able to create functions from scratch in python\n",
    "- Have a basic understanding of linear algebra concepts such as matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction: What is Dimensionality Reduction? \n",
    "\n",
    "Dimensionality reduction reduces the number of random variables that you are considering for analysis until you are left with the most important variables.\n",
    "\n",
    "Dimensionality reduction is not an end goal in itself, but a tool to form a dataset with more parsimonious features for further visualization and/or modelling.\n",
    "\n",
    "> Check: where have we already done dimensionality reduction? What are the potential benefits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine we have a linear graph, with one variable on the x axis and another on the y axis. Fitting a line models most of the information in the data (but leaves some noise). We can reduce the dimensions until the 45 degree line is completely horizontal - both of our measurements are now on the same plane - they are *one-dimensional*.\n",
    "\n",
    "![graph1](./assets/images/graph1.jpg)\n",
    "\n",
    "![graph2](./assets/images/graph2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So our goal is to reduce dimensions without losing information.\n",
    "\n",
    "In other words, to remove redundancies in our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.153276</td>\n",
       "      <td>0.999527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.153276</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.163417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999527</td>\n",
       "      <td>-0.163417</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000 -0.153276  0.999527\n",
       "1 -0.153276  1.000000 -0.163417\n",
       "2  0.999527 -0.163417  1.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "x1 = [np.random.randn() for i in range(20)]\n",
    "x2 = [np.random.randn() for i in range(20)]\n",
    "x3 = [x*3 + np.random.randn()/10 for x in x1]\n",
    "pd.DataFrame(zip(x1, x2, x3)).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where might there be redundant information here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A refresher on covariance and correlation\n",
    "\n",
    "What is covariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Covariance is a measure of how two variables covary (i.e. how much the change in one is associated with the change in the other).  It specifically looks at how variables covary linearly.\n",
    "\n",
    "$$\n",
    "\\text{COV}(X,Y) =  \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n}\n",
    "$$\n",
    "\n",
    "\n",
    "Can take positive and negative values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Normalized covariance.  \n",
    "\n",
    "$$\n",
    "\\text{corr}(X,Y) =  \\frac{COV(X,Y)}{\\sigma_x \\sigma_y}\n",
    "$$\n",
    "\n",
    "Can take values from -1 to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we care about covariance?\n",
    "\n",
    "It incorporates the signal (variance) and also redundancy.\n",
    "\n",
    "Covariance is calculated between two variables.  What if we have many variables?\n",
    "\n",
    "We can calculate a covariance matrix (similar to the correlation matrix we've seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.126418</td>\n",
       "      <td>-0.180250</td>\n",
       "      <td>3.378774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.180250</td>\n",
       "      <td>1.227733</td>\n",
       "      <td>-0.576718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.378774</td>\n",
       "      <td>-0.576718</td>\n",
       "      <td>10.144468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1          2\n",
       "0  1.126418 -0.180250   3.378774\n",
       "1 -0.180250  1.227733  -0.576718\n",
       "2  3.378774 -0.576718  10.144468"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(x1, x2, x3)).cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The values on the diagonal are just variance since $COV(X,X) = VAR(X)$.\n",
    "\n",
    "What would an 'ideal' covariance matrix look like?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An \"ideal\" covariance matrix for data would have large numbers (variances) along the diagonal because this would indicate a large amount of signal in the data. It would also have zero values in the off-diagonal elements because these values indicate redundancy across our variables.\n",
    "\n",
    "What can we do to try to remove any redundancies and preserve the signal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Enter PCA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Demo: Applications of Dimensionality Reduction\n",
    "\n",
    "Our first priority is to get comfortable with the initial manual workflow of PCA. (We'll expand in a following lesson.)\n",
    "\n",
    "- Isolate the feature data\n",
    "- Center and scale the feature data\n",
    "- Calculate their covariance matrix\n",
    "- Calculate the eigenvalues and eigenvectors\n",
    "- Choose the best n principal components\n",
    "- Calculate newly extracted feature data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "x = data.ix[selection].values\n",
    "y = data.ix[selection].values\n",
    "x_standard = StandardScaler().fit_transform(x)\n",
    "\n",
    "```\n",
    "\n",
    "A **covariance matrix** of n-features is just an n x n matrix, where the elements are the [covariances](https://en.wikipedia.org/wiki/Covariance) for each pair of _n_ features.\n",
    "\n",
    "```\n",
    "cov_mat = np.cov(x_standard.T)\n",
    "```\n",
    "\n",
    "(We're **transposing** the matrix only because np.cov expects features to be on the rows and columns to hold observations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we decompose our matrix by calling the numpy linear algebra function ```linalg.eig()```. to calculate the [**eigenvectors** and **eigenvalues**](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).\n",
    "\n",
    "```\n",
    "eigenValues, eigenVectors = np.linalg.eig(cov_mat)\n",
    "```\n",
    "\n",
    "The eigenvectors of a linear transformation are vectors that do not change direction under that transformation, but only have their magnitude scaled by some scalar value (the eigenvalue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this context, the eigenvectors are the new dimensions of our data.  These are the principal components.\n",
    "\n",
    "The larger an eigenvalue, the more variance (information) in our data its corresponding eigenvector explains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once we have our eigenvalues, we can work on transforming our data onto another dimensional space. Remember the visual representation from above - this is exactly what we are doing in this step. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided Practice: Conducting Dimensionality Analysis\n",
    "\n",
    "Now that you know the procedure, let's run through an implementation of dimensionality reduction with a real dataset.\n",
    "\n",
    "We're going to be revisiting the [wine](./assets/datasets/wine_v.csv) dataset that lists the attributes of various different wine varieties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Varietal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>Cabernet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>Cabernet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>Cabernet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>Cabernet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>Cabernet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  Varietal  \n",
       "0      9.4        5  Cabernet  \n",
       "1      9.8        5  Cabernet  \n",
       "2      9.8        5  Cabernet  \n",
       "3      9.8        6  Cabernet  \n",
       "4      9.4        5  Cabernet  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv('./assets/datasets/wine_v.csv')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Isolate the feature data.\"\"\"\n",
    "x = wine.ix[:,0:11].values\n",
    "y = wine.ix[:,12].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the covariace matrix for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.031416</td>\n",
       "      <td>-0.079851</td>\n",
       "      <td>0.227820</td>\n",
       "      <td>0.281756</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>-2.800921</td>\n",
       "      <td>-6.482346</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>-0.183586</td>\n",
       "      <td>0.054010</td>\n",
       "      <td>-0.114421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.079851</td>\n",
       "      <td>0.032062</td>\n",
       "      <td>-0.019272</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>-0.019674</td>\n",
       "      <td>0.450426</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>-0.007921</td>\n",
       "      <td>-0.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.227820</td>\n",
       "      <td>-0.019272</td>\n",
       "      <td>0.037947</td>\n",
       "      <td>0.039434</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>-0.124252</td>\n",
       "      <td>0.227697</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>-0.016298</td>\n",
       "      <td>0.010328</td>\n",
       "      <td>0.022815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.281756</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.039434</td>\n",
       "      <td>1.987897</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>2.758611</td>\n",
       "      <td>9.416441</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>-0.018644</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.063219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007679</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.073387</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.001926</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>-0.011092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-2.800921</td>\n",
       "      <td>-0.019674</td>\n",
       "      <td>-0.124252</td>\n",
       "      <td>2.758611</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>109.414884</td>\n",
       "      <td>229.737521</td>\n",
       "      <td>-0.000433</td>\n",
       "      <td>0.113653</td>\n",
       "      <td>0.091592</td>\n",
       "      <td>-0.773698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-6.482346</td>\n",
       "      <td>0.450426</td>\n",
       "      <td>0.227697</td>\n",
       "      <td>9.416441</td>\n",
       "      <td>0.073387</td>\n",
       "      <td>229.737521</td>\n",
       "      <td>1082.102373</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>-0.337699</td>\n",
       "      <td>0.239471</td>\n",
       "      <td>-7.209298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000433</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.183586</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>-0.016298</td>\n",
       "      <td>-0.018644</td>\n",
       "      <td>-0.001926</td>\n",
       "      <td>0.113653</td>\n",
       "      <td>-0.337699</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>0.023835</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>0.033832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.054010</td>\n",
       "      <td>-0.007921</td>\n",
       "      <td>0.010328</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>0.091592</td>\n",
       "      <td>0.239471</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>0.028733</td>\n",
       "      <td>0.016907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.114421</td>\n",
       "      <td>-0.038600</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.063219</td>\n",
       "      <td>-0.011092</td>\n",
       "      <td>-0.773698</td>\n",
       "      <td>-7.209298</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>0.033832</td>\n",
       "      <td>0.016907</td>\n",
       "      <td>1.135647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4           5            6   \\\n",
       "0   3.031416 -0.079851  0.227820  0.281756  0.007679   -2.800921    -6.482346   \n",
       "1  -0.079851  0.032062 -0.019272  0.000484  0.000517   -0.019674     0.450426   \n",
       "2   0.227820 -0.019272  0.037947  0.039434  0.001869   -0.124252     0.227697   \n",
       "3   0.281756  0.000484  0.039434  1.987897  0.003690    2.758611     9.416441   \n",
       "4   0.007679  0.000517  0.001869  0.003690  0.002215    0.002738     0.073387   \n",
       "5  -2.800921 -0.019674 -0.124252  2.758611  0.002738  109.414884   229.737521   \n",
       "6  -6.482346  0.450426  0.227697  9.416441  0.073387  229.737521  1082.102373   \n",
       "7   0.002195  0.000007  0.000134  0.000945  0.000018   -0.000433     0.004425   \n",
       "8  -0.183586  0.006495 -0.016298 -0.018644 -0.001926    0.113653    -0.337699   \n",
       "9   0.054010 -0.007921  0.010328  0.001321  0.002962    0.091592     0.239471   \n",
       "10 -0.114421 -0.038600  0.022815  0.063219 -0.011092   -0.773698    -7.209298   \n",
       "\n",
       "          7         8         9         10  \n",
       "0   0.002195 -0.183586  0.054010 -0.114421  \n",
       "1   0.000007  0.006495 -0.007921 -0.038600  \n",
       "2   0.000134 -0.016298  0.010328  0.022815  \n",
       "3   0.000945 -0.018644  0.001321  0.063219  \n",
       "4   0.000018 -0.001926  0.002962 -0.011092  \n",
       "5  -0.000433  0.113653  0.091592 -0.773698  \n",
       "6   0.004425 -0.337699  0.239471 -7.209298  \n",
       "7   0.000004 -0.000100  0.000048 -0.000998  \n",
       "8  -0.000100  0.023835 -0.005146  0.033832  \n",
       "9   0.000048 -0.005146  0.028733  0.016907  \n",
       "10 -0.000998  0.033832  0.016907  1.135647  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.cov(x.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000626</td>\n",
       "      <td>-0.256291</td>\n",
       "      <td>0.672124</td>\n",
       "      <td>0.114849</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>-0.153890</td>\n",
       "      <td>-0.113252</td>\n",
       "      <td>0.668465</td>\n",
       "      <td>-0.683406</td>\n",
       "      <td>0.183120</td>\n",
       "      <td>-0.061707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.256291</td>\n",
       "      <td>1.000626</td>\n",
       "      <td>-0.552841</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.061336</td>\n",
       "      <td>-0.010510</td>\n",
       "      <td>0.076518</td>\n",
       "      <td>0.022040</td>\n",
       "      <td>0.235084</td>\n",
       "      <td>-0.261150</td>\n",
       "      <td>-0.202415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.672124</td>\n",
       "      <td>-0.552841</td>\n",
       "      <td>1.000626</td>\n",
       "      <td>0.143667</td>\n",
       "      <td>0.203950</td>\n",
       "      <td>-0.061016</td>\n",
       "      <td>0.035555</td>\n",
       "      <td>0.365176</td>\n",
       "      <td>-0.542243</td>\n",
       "      <td>0.312966</td>\n",
       "      <td>0.109972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.114849</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.143667</td>\n",
       "      <td>1.000626</td>\n",
       "      <td>0.055644</td>\n",
       "      <td>0.187166</td>\n",
       "      <td>0.203155</td>\n",
       "      <td>0.355506</td>\n",
       "      <td>-0.085706</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>0.042102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.061336</td>\n",
       "      <td>0.203950</td>\n",
       "      <td>0.055644</td>\n",
       "      <td>1.000626</td>\n",
       "      <td>0.005566</td>\n",
       "      <td>0.047430</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>-0.265192</td>\n",
       "      <td>0.371493</td>\n",
       "      <td>-0.221279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.153890</td>\n",
       "      <td>-0.010510</td>\n",
       "      <td>-0.061016</td>\n",
       "      <td>0.187166</td>\n",
       "      <td>0.005566</td>\n",
       "      <td>1.000626</td>\n",
       "      <td>0.668084</td>\n",
       "      <td>-0.021960</td>\n",
       "      <td>0.070422</td>\n",
       "      <td>0.051690</td>\n",
       "      <td>-0.069452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.113252</td>\n",
       "      <td>0.076518</td>\n",
       "      <td>0.035555</td>\n",
       "      <td>0.203155</td>\n",
       "      <td>0.047430</td>\n",
       "      <td>0.668084</td>\n",
       "      <td>1.000626</td>\n",
       "      <td>0.071314</td>\n",
       "      <td>-0.066536</td>\n",
       "      <td>0.042974</td>\n",
       "      <td>-0.205783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.668465</td>\n",
       "      <td>0.022040</td>\n",
       "      <td>0.365176</td>\n",
       "      <td>0.355506</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>-0.021960</td>\n",
       "      <td>0.071314</td>\n",
       "      <td>1.000626</td>\n",
       "      <td>-0.341913</td>\n",
       "      <td>0.148599</td>\n",
       "      <td>-0.496490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.683406</td>\n",
       "      <td>0.235084</td>\n",
       "      <td>-0.542243</td>\n",
       "      <td>-0.085706</td>\n",
       "      <td>-0.265192</td>\n",
       "      <td>0.070422</td>\n",
       "      <td>-0.066536</td>\n",
       "      <td>-0.341913</td>\n",
       "      <td>1.000626</td>\n",
       "      <td>-0.196771</td>\n",
       "      <td>0.205761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.183120</td>\n",
       "      <td>-0.261150</td>\n",
       "      <td>0.312966</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>0.371493</td>\n",
       "      <td>0.051690</td>\n",
       "      <td>0.042974</td>\n",
       "      <td>0.148599</td>\n",
       "      <td>-0.196771</td>\n",
       "      <td>1.000626</td>\n",
       "      <td>0.093653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.061707</td>\n",
       "      <td>-0.202415</td>\n",
       "      <td>0.109972</td>\n",
       "      <td>0.042102</td>\n",
       "      <td>-0.221279</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>-0.205783</td>\n",
       "      <td>-0.496490</td>\n",
       "      <td>0.205761</td>\n",
       "      <td>0.093653</td>\n",
       "      <td>1.000626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000626 -0.256291  0.672124  0.114849  0.093764 -0.153890 -0.113252   \n",
       "1  -0.256291  1.000626 -0.552841  0.001919  0.061336 -0.010510  0.076518   \n",
       "2   0.672124 -0.552841  1.000626  0.143667  0.203950 -0.061016  0.035555   \n",
       "3   0.114849  0.001919  0.143667  1.000626  0.055644  0.187166  0.203155   \n",
       "4   0.093764  0.061336  0.203950  0.055644  1.000626  0.005566  0.047430   \n",
       "5  -0.153890 -0.010510 -0.061016  0.187166  0.005566  1.000626  0.668084   \n",
       "6  -0.113252  0.076518  0.035555  0.203155  0.047430  0.668084  1.000626   \n",
       "7   0.668465  0.022040  0.365176  0.355506  0.200758 -0.021960  0.071314   \n",
       "8  -0.683406  0.235084 -0.542243 -0.085706 -0.265192  0.070422 -0.066536   \n",
       "9   0.183120 -0.261150  0.312966  0.005531  0.371493  0.051690  0.042974   \n",
       "10 -0.061707 -0.202415  0.109972  0.042102 -0.221279 -0.069452 -0.205783   \n",
       "\n",
       "          7         8         9         10  \n",
       "0   0.668465 -0.683406  0.183120 -0.061707  \n",
       "1   0.022040  0.235084 -0.261150 -0.202415  \n",
       "2   0.365176 -0.542243  0.312966  0.109972  \n",
       "3   0.355506 -0.085706  0.005531  0.042102  \n",
       "4   0.200758 -0.265192  0.371493 -0.221279  \n",
       "5  -0.021960  0.070422  0.051690 -0.069452  \n",
       "6   0.071314 -0.066536  0.042974 -0.205783  \n",
       "7   1.000626 -0.341913  0.148599 -0.496490  \n",
       "8  -0.341913  1.000626 -0.196771  0.205761  \n",
       "9   0.148599 -0.196771  1.000626  0.093653  \n",
       "10 -0.496490  0.205761  0.093653  1.000626  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Center and scale the feature data.\"\"\"\n",
    "x_standard = StandardScaler().fit_transform(x)\n",
    "\n",
    "\"\"\" Calculate their covariance matrix. \"\"\"\n",
    "cov_mat = np.cov(x_standard.T)\n",
    "\n",
    "pd.DataFrame(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the eigenvalues and eigenvectors.\"\"\"\n",
    "eigenValues, eigenVectors = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.1010718226758938, array([ 0.48931422, -0.23858436,  0.46363166,  0.14610715,  0.21224658,\n",
      "       -0.03615752,  0.02357485,  0.39535301, -0.43851962,  0.24292133,\n",
      "       -0.11323207]))\n",
      "(1.9271148896490469, array([-0.11050274,  0.27493048, -0.15179136,  0.27208024,  0.14805156,\n",
      "        0.51356681,  0.56948696,  0.23357549,  0.00671079, -0.03755392,\n",
      "       -0.38618096]))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Choose the best n principal components.  Calculate newly extracted feature data.\"\"\"\n",
    "\n",
    "eig_pairs = [(np.abs(eigenValues[i]), eigenVectors[:,i])\\\n",
    "             for i in range(len(eigenValues))]\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "for i in eig_pairs[:2]:\n",
    "    print(i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now what?\n",
    "\n",
    "We can use this to transform our data onto a lower dimension space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48931422, -0.11050274],\n",
       "       [-0.23858436,  0.27493048],\n",
       "       [ 0.46363166, -0.15179136],\n",
       "       [ 0.14610715,  0.27208024],\n",
       "       [ 0.21224658,  0.14805156],\n",
       "       [-0.03615752,  0.51356681],\n",
       "       [ 0.02357485,  0.56948696],\n",
       "       [ 0.39535301,  0.23357549],\n",
       "       [-0.43851962,  0.00671079],\n",
       "       [ 0.24292133, -0.03755392],\n",
       "       [-0.11323207, -0.38618096]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.hstack((eig_pairs[0][1].reshape(11,1), \\\n",
    "               eig_pairs[1][1].reshape(11,1))) # Our transformation matrix\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.61952988,  0.45095009],\n",
       "       [-0.79916993,  1.85655306],\n",
       "       [-0.74847909,  0.88203886],\n",
       "       ..., \n",
       "       [-1.45612897,  0.31174559],\n",
       "       [-2.27051793,  0.97979111],\n",
       "       [-0.42697475, -0.53669021]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced = x_standard.dot(W)\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This matrix represents the original data transformed into the two-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the covariance matrix again.  \n",
    "\n",
    "Remember we now only have two features.\n",
    "\n",
    "Is it better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.101072e+00</td>\n",
       "      <td>-6.280611e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.280611e-16</td>\n",
       "      <td>1.927115e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1\n",
       "0  3.101072e+00 -6.280611e-16\n",
       "1 -6.280611e-16  1.927115e+00"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat = np.cov(X_reduced.T)\n",
    "\n",
    "pd.DataFrame(cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How else can we see how well we managed to keep the signal, but remove the redundancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fraction of variance explained by our new features!\n",
    "\n",
    "This is calculated using the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  28.17393128   45.68220118   59.77805108   70.80743772   79.52827474\n",
      "   85.52471351   90.83190641   94.67696732   97.81007747   99.4585608   100.        ]\n"
     ]
    }
   ],
   "source": [
    "tot = sum(eigenValues)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eigenValues, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print cum_var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remember, this was just pre-processing.  We can build models in the same way with the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.617424242424 mean accuracy, using 11 dimensions.\n",
      "0.513257575758 mean accuracy, using 2 principal component dimensions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_standard, y, test_size=0.33, random_state=1)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print clf.score(X_test, y_test), \"mean accuracy, using {0} dimensions.\".format(x_standard.shape[1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.33, random_state=1)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print clf.score(X_test, y_test), \"mean accuracy, using {0} principal component dimensions.\".format(X_reduced.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent Practice: Dimensionality Reduction on the Iris dataset\n",
    "Now that we've gone over the long-form approach to dimensionality reduction and worked through an example, let's put your skills to the test! We're going to be working with the classic [iris dataset](./assets/datasets/iris.csv). We want to decompose the data to the point of finding the eigenvectors and eigenvalues. Grab the [starter code](./code/starter-code/w7d2-dimensionality-reduction-iris-starter-code.ipynb) to begin!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 mins)\n",
    "- Recap and recall the process steps in dimensionality reduction\n",
    "    -  Covariance Matrix: First, we create a covariance matrix to decompose so that we may find our eigenvalues / eigenvectors. \n",
    "    -  Eigenvectors & Eigenvalues: We decompose the covariance matrix to derive our eigenvectors and eigenvalues, and select the top  combined eigenpairs to become our principal components.\n",
    "    -  Lastly, we project the eigenpairs onto a new feature subspace.\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Unsupervised Dimensionality Reduction in sklearn](http://scikit-learn.org/stable/modules/unsupervised_reduction.html)\n",
    "- [In depth overview of Dimensionality Reduction and PCA from Stanford University](http://ufldl.stanford.edu/wiki/index.php/PCA)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
